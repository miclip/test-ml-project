{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Experiments: Tree-Based Classifiers for Churn Prediction\n",
    "\n",
    "This notebook trains and compares tree-based models against the baseline Logistic Regression.\n",
    "\n",
    "## Objectives\n",
    "- Train Random Forest with hyperparameter tuning\n",
    "- Train Gradient Boosting with hyperparameter tuning\n",
    "- Use cross-validation for robust evaluation\n",
    "- Log all experiments to MLflow\n",
    "- Compare against baseline and select best model\n",
    "\n",
    "## Success Criteria\n",
    "- **Target:** AUC-ROC > 0.75 on validation set\n",
    "- **Baseline to beat:** Logistic Regression AUC-ROC (from Phase 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e .. --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    make_scorer\n",
    ")\n",
    "\n",
    "# Import our custom modules\n",
    "from src.data_loader import create_sample_data\n",
    "from src.preprocessing import (\n",
    "    ChurnPreprocessor, \n",
    "    create_train_val_test_split,\n",
    "    prepare_features_and_target\n",
    ")\n",
    "from src.experiment import (\n",
    "    setup_experiment,\n",
    "    start_run,\n",
    "    log_params,\n",
    "    log_metrics,\n",
    "    log_classification_metrics,\n",
    "    log_dataset_info,\n",
    "    log_model,\n",
    "    get_best_run\n",
    ")\n",
    "\n",
    "# Display settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data with realistic correlations\n",
    "df = create_sample_data(n_samples=1000, random_state=RANDOM_STATE, churn_rate=0.2)\n",
    "\n",
    "# Split data into train/validation/test sets\n",
    "train_df, val_df, test_df = create_train_val_test_split(\n",
    "    df, \n",
    "    target_column='churn',\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.15,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)} (held out)\")\n",
    "print(f\"\\nChurn rate: {df['churn'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor and fit on training data\n",
    "preprocessor = ChurnPreprocessor()\n",
    "\n",
    "# Fit and transform training data\n",
    "train_preprocessed = preprocessor.fit_transform(train_df, target_column='churn')\n",
    "X_train, y_train = prepare_features_and_target(train_preprocessed, target_column='churn')\n",
    "\n",
    "# Transform validation data\n",
    "val_preprocessed = preprocessor.transform(val_df)\n",
    "X_val, y_val = prepare_features_and_target(val_preprocessed, target_column='churn')\n",
    "\n",
    "# Convert to numpy arrays\n",
    "feature_names = list(X_train.columns)\n",
    "X_train = X_train.values\n",
    "X_val = X_val.values\n",
    "y_train = y_train.values\n",
    "y_val = y_val.values\n",
    "\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Validation features shape: {X_val.shape}\")\n",
    "print(f\"Features: {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up MLflow Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MLflow experiment for model comparison\n",
    "experiment_name = \"churn_prediction_model_comparison\"\n",
    "experiment_id = setup_experiment(experiment_name)\n",
    "\n",
    "print(f\"MLflow Experiment: {experiment_name}\")\n",
    "print(f\"Experiment ID: {experiment_id}\")\n",
    "\n",
    "# Store results for comparison\n",
    "model_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline: Logistic Regression (Reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline Logistic Regression for comparison\n",
    "with start_run(run_name=\"logistic_regression_baseline\") as run:\n",
    "    # Train model\n",
    "    lr_model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_val_pred = lr_model.predict(X_val)\n",
    "    y_val_prob = lr_model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(lr_model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    \n",
    "    # Log parameters and metrics\n",
    "    log_params({\"model_type\": \"LogisticRegression\", \"C\": 1.0, \"penalty\": \"l2\"})\n",
    "    val_metrics = log_classification_metrics(y_val, y_val_pred, y_val_prob, prefix=\"val_\")\n",
    "    log_metrics({\"cv_roc_auc_mean\": cv_scores.mean(), \"cv_roc_auc_std\": cv_scores.std()})\n",
    "    \n",
    "    lr_run_id = run.info.run_id\n",
    "    \n",
    "    model_results.append({\n",
    "        'model': 'Logistic Regression',\n",
    "        'run_id': lr_run_id,\n",
    "        'val_accuracy': val_metrics['val_accuracy'],\n",
    "        'val_precision': val_metrics['val_precision'],\n",
    "        'val_recall': val_metrics['val_recall'],\n",
    "        'val_f1': val_metrics['val_f1_score'],\n",
    "        'val_roc_auc': val_metrics['val_roc_auc'],\n",
    "        'cv_roc_auc': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std()\n",
    "    })\n",
    "    \n",
    "print(f\"Logistic Regression - Val AUC: {val_metrics['val_roc_auc']:.4f}, CV AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid for Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "print(\"Random Forest Hyperparameter Grid:\")\n",
    "for param, values in rf_param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "total_combinations = 1\n",
    "for values in rf_param_grid.values():\n",
    "    total_combinations *= len(values)\n",
    "print(f\"\\nTotal combinations: {total_combinations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Grid Search with Cross-Validation\n",
    "print(\"Running GridSearchCV for Random Forest (this may take a few minutes)...\")\n",
    "\n",
    "rf_base = RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "rf_grid_search = GridSearchCV(\n",
    "    rf_base,\n",
    "    rf_param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {rf_grid_search.best_params_}\")\n",
    "print(f\"Best CV AUC: {rf_grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log best Random Forest model to MLflow\n",
    "with start_run(run_name=\"random_forest_tuned\") as run:\n",
    "    # Get best model\n",
    "    rf_model = rf_grid_search.best_estimator_\n",
    "    \n",
    "    # Predictions\n",
    "    y_val_pred = rf_model.predict(X_val)\n",
    "    y_val_prob = rf_model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Log parameters\n",
    "    log_params({\n",
    "        \"model_type\": \"RandomForest\",\n",
    "        **rf_grid_search.best_params_\n",
    "    })\n",
    "    \n",
    "    # Log metrics\n",
    "    val_metrics = log_classification_metrics(y_val, y_val_pred, y_val_prob, prefix=\"val_\")\n",
    "    log_metrics({\n",
    "        \"cv_roc_auc_mean\": rf_grid_search.best_score_,\n",
    "        \"cv_roc_auc_std\": rf_grid_search.cv_results_['std_test_score'][rf_grid_search.best_index_]\n",
    "    })\n",
    "    \n",
    "    # Log model\n",
    "    log_model(rf_model, artifact_path=\"model\")\n",
    "    \n",
    "    rf_run_id = run.info.run_id\n",
    "    \n",
    "    model_results.append({\n",
    "        'model': 'Random Forest',\n",
    "        'run_id': rf_run_id,\n",
    "        'val_accuracy': val_metrics['val_accuracy'],\n",
    "        'val_precision': val_metrics['val_precision'],\n",
    "        'val_recall': val_metrics['val_recall'],\n",
    "        'val_f1': val_metrics['val_f1_score'],\n",
    "        'val_roc_auc': val_metrics['val_roc_auc'],\n",
    "        'cv_roc_auc': rf_grid_search.best_score_,\n",
    "        'cv_std': rf_grid_search.cv_results_['std_test_score'][rf_grid_search.best_index_]\n",
    "    })\n",
    "    \n",
    "print(f\"Random Forest - Val AUC: {val_metrics['val_roc_auc']:.4f}, CV AUC: {rf_grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid for Gradient Boosting\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "print(\"Gradient Boosting Hyperparameter Grid:\")\n",
    "for param, values in gb_param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "total_combinations = 1\n",
    "for values in gb_param_grid.values():\n",
    "    total_combinations *= len(values)\n",
    "print(f\"\\nTotal combinations: {total_combinations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Grid Search with Cross-Validation\n",
    "print(\"Running GridSearchCV for Gradient Boosting (this may take a few minutes)...\")\n",
    "\n",
    "gb_base = GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "gb_grid_search = GridSearchCV(\n",
    "    gb_base,\n",
    "    gb_param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "gb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {gb_grid_search.best_params_}\")\n",
    "print(f\"Best CV AUC: {gb_grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log best Gradient Boosting model to MLflow\n",
    "with start_run(run_name=\"gradient_boosting_tuned\") as run:\n",
    "    # Get best model\n",
    "    gb_model = gb_grid_search.best_estimator_\n",
    "    \n",
    "    # Predictions\n",
    "    y_val_pred = gb_model.predict(X_val)\n",
    "    y_val_prob = gb_model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Log parameters\n",
    "    log_params({\n",
    "        \"model_type\": \"GradientBoosting\",\n",
    "        **gb_grid_search.best_params_\n",
    "    })\n",
    "    \n",
    "    # Log metrics\n",
    "    val_metrics = log_classification_metrics(y_val, y_val_pred, y_val_prob, prefix=\"val_\")\n",
    "    log_metrics({\n",
    "        \"cv_roc_auc_mean\": gb_grid_search.best_score_,\n",
    "        \"cv_roc_auc_std\": gb_grid_search.cv_results_['std_test_score'][gb_grid_search.best_index_]\n",
    "    })\n",
    "    \n",
    "    # Log model\n",
    "    log_model(gb_model, artifact_path=\"model\")\n",
    "    \n",
    "    gb_run_id = run.info.run_id\n",
    "    \n",
    "    model_results.append({\n",
    "        'model': 'Gradient Boosting',\n",
    "        'run_id': gb_run_id,\n",
    "        'val_accuracy': val_metrics['val_accuracy'],\n",
    "        'val_precision': val_metrics['val_precision'],\n",
    "        'val_recall': val_metrics['val_recall'],\n",
    "        'val_f1': val_metrics['val_f1_score'],\n",
    "        'val_roc_auc': val_metrics['val_roc_auc'],\n",
    "        'cv_roc_auc': gb_grid_search.best_score_,\n",
    "        'cv_std': gb_grid_search.cv_results_['std_test_score'][gb_grid_search.best_index_]\n",
    "    })\n",
    "    \n",
    "print(f\"Gradient Boosting - Val AUC: {val_metrics['val_roc_auc']:.4f}, CV AUC: {gb_grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame(model_results)\n",
    "results_df = results_df.sort_values('val_roc_auc', ascending=False)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL COMPARISON - Sorted by Validation AUC-ROC\")\n",
    "print(\"=\"*80)\n",
    "print(results_df[['model', 'val_roc_auc', 'cv_roc_auc', 'cv_std', 'val_f1', 'val_precision', 'val_recall']].to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# AUC-ROC comparison\n",
    "ax1 = axes[0]\n",
    "x = range(len(results_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar([i - width/2 for i in x], results_df['val_roc_auc'], width, label='Validation AUC', color='steelblue')\n",
    "bars2 = ax1.bar([i + width/2 for i in x], results_df['cv_roc_auc'], width, label='CV AUC (mean)', color='lightcoral')\n",
    "\n",
    "ax1.axhline(y=0.75, color='green', linestyle='--', linewidth=2, label='Target (0.75)')\n",
    "ax1.set_xlabel('Model')\n",
    "ax1.set_ylabel('AUC-ROC')\n",
    "ax1.set_title('Model Comparison: AUC-ROC Scores')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(results_df['model'], rotation=15)\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0.5, 1.0)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Precision/Recall comparison\n",
    "ax2 = axes[1]\n",
    "bars3 = ax2.bar([i - width/2 for i in x], results_df['val_precision'], width, label='Precision', color='forestgreen')\n",
    "bars4 = ax2.bar([i + width/2 for i in x], results_df['val_recall'], width, label='Recall', color='darkorange')\n",
    "\n",
    "ax2.set_xlabel('Model')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('Model Comparison: Precision vs Recall')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(results_df['model'], rotation=15)\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "models = [\n",
    "    ('Logistic Regression', lr_model),\n",
    "    ('Random Forest', rf_model),\n",
    "    ('Gradient Boosting', gb_model)\n",
    "]\n",
    "\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "for (name, model), color in zip(models, colors):\n",
    "    y_prob = model.predict_proba(X_val)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_prob)\n",
    "    auc = roc_auc_score(y_val, y_prob)\n",
    "    ax.plot(fpr, tpr, color=color, lw=2, label=f'{name} (AUC = {auc:.3f})')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curves Comparison', fontsize=14)\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Select Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine best model based on validation AUC-ROC\n",
    "best_model_row = results_df.iloc[0]\n",
    "best_model_name = best_model_row['model']\n",
    "best_model_auc = best_model_row['val_roc_auc']\n",
    "best_run_id = best_model_row['run_id']\n",
    "\n",
    "# Get the actual model object\n",
    "if best_model_name == 'Random Forest':\n",
    "    best_model = rf_model\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    best_model = gb_model\n",
    "else:\n",
    "    best_model = lr_model\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BEST MODEL SELECTED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: {best_model_name}\")\n",
    "print(f\"Validation AUC-ROC: {best_model_auc:.4f}\")\n",
    "print(f\"MLflow Run ID: {best_run_id}\")\n",
    "\n",
    "# Check against target\n",
    "target_auc = 0.75\n",
    "if best_model_auc >= target_auc:\n",
    "    print(f\"\\n[PASS] Best model meets target AUC-ROC >= {target_auc}\")\n",
    "else:\n",
    "    print(f\"\\n[FAIL] Best model does not meet target AUC-ROC >= {target_auc}\")\n",
    "    print(f\"       Need {target_auc - best_model_auc:.4f} improvement\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from tree-based models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Random Forest feature importance\n",
    "rf_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.barh(rf_importance['feature'], rf_importance['importance'], color='forestgreen', alpha=0.7)\n",
    "ax1.set_xlabel('Feature Importance')\n",
    "ax1.set_title('Random Forest Feature Importance')\n",
    "\n",
    "# Gradient Boosting feature importance\n",
    "gb_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': gb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.barh(gb_importance['feature'], gb_importance['importance'], color='darkorange', alpha=0.7)\n",
    "ax2.set_xlabel('Feature Importance')\n",
    "ax2.set_title('Gradient Boosting Feature Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top features from best model\n",
    "if best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop Features for {best_model_name}:\")\n",
    "    print(importance_df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nFeature Importance Insights:\")\n",
    "    top_features = importance_df.head(3)['feature'].tolist()\n",
    "    print(f\"  - Top 3 predictive features: {', '.join(top_features)}\")\n",
    "    print(f\"  - These features should align with domain knowledge about churn drivers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 3 MODEL EXPERIMENTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n--- Models Trained ---\")\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"  {row['model']:25s} Val AUC: {row['val_roc_auc']:.4f}  CV AUC: {row['cv_roc_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\n--- Best Model ---\")\n",
    "print(f\"  Model: {best_model_name}\")\n",
    "print(f\"  Validation AUC-ROC: {best_model_auc:.4f}\")\n",
    "print(f\"  Target (0.75): {'ACHIEVED' if best_model_auc >= 0.75 else 'NOT YET ACHIEVED'}\")\n",
    "\n",
    "print(\"\\n--- Next Steps ---\")\n",
    "print(\"  1. Evaluate best model on held-out TEST set (one-time evaluation)\")\n",
    "print(\"  2. Generate final confusion matrix and ROC curve\")\n",
    "print(\"  3. Document feature importance for stakeholders\")\n",
    "print(\"  4. Create model card for deployment\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
