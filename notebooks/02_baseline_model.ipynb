{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model: Logistic Regression for Customer Churn Prediction\n",
    "\n",
    "This notebook establishes a baseline model using Logistic Regression for predicting customer churn.\n",
    "\n",
    "## Objectives\n",
    "- Define success criteria for the baseline model\n",
    "- Train Logistic Regression with default parameters\n",
    "- Log hyperparameters and metrics to MLflow\n",
    "- Evaluate on validation set\n",
    "- Generate confusion matrix and ROC curve\n",
    "\n",
    "## Success Criteria\n",
    "The baseline model should achieve:\n",
    "- **AUC-ROC > 0.60** on validation set (minimum viable baseline)\n",
    "- This establishes the performance floor that improved models must beat\n",
    "- Target for final model: **AUC-ROC > 0.75**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -e .. --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# Import our custom modules\n",
    "from src.data_loader import create_sample_data\n",
    "from src.preprocessing import ChurnPreprocessor, create_train_val_test_split\n",
    "from src.experiment import (\n",
    "    setup_experiment,\n",
    "    start_run,\n",
    "    log_params,\n",
    "    log_classification_metrics,\n",
    "    log_dataset_info,\n",
    "    log_model,\n",
    "    log_figure\n",
    ")\n",
    "\n",
    "# Display settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1000, 7)\n",
      "\n",
      "Churn distribution:\n",
      "churn\n",
      "0    0.8\n",
      "1    0.2\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Generate sample data with realistic correlations\n",
    "df = create_sample_data(n_samples=1000, random_state=42, churn_rate=0.2)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nChurn distribution:\")\n",
    "print(df['churn'].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 700 samples\n",
      "Validation set: 150 samples\n",
      "Test set: 150 samples (held out for final evaluation)\n"
     ]
    }
   ],
   "source": [
    "# Split data into train/validation/test sets\n",
    "train_df, val_df, test_df = create_train_val_test_split(\n",
    "    df, \n",
    "    target_column='churn',\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(val_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples (held out for final evaluation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize preprocessor and fit on training data\nfrom src.preprocessing import prepare_features_and_target\n\npreprocessor = ChurnPreprocessor()\n\n# Fit and transform training data\ntrain_preprocessed = preprocessor.fit_transform(train_df, target_column='churn')\nX_train, y_train = prepare_features_and_target(train_preprocessed, target_column='churn')\n\n# Transform validation data using fitted preprocessor\nval_preprocessed = preprocessor.transform(val_df)\nX_val, y_val = prepare_features_and_target(val_preprocessed, target_column='churn')\n\n# Convert to numpy arrays for sklearn\nX_train = X_train.values\nX_val = X_val.values\ny_train = y_train.values\ny_val = y_val.values\n\nprint(f\"Training features shape: {X_train.shape}\")\nprint(f\"Validation features shape: {X_val.shape}\")\nprint(f\"\\nFeature columns: {list(train_preprocessed.columns)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up MLflow Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MLflow experiment\n",
    "experiment_name = \"churn_prediction_baseline\"\n",
    "experiment_id = setup_experiment(experiment_name)\n",
    "\n",
    "print(f\"MLflow Experiment: {experiment_name}\")\n",
    "print(f\"Experiment ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Baseline Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow run\n",
    "with start_run(run_name=\"logistic_regression_baseline\", \n",
    "               description=\"Baseline Logistic Regression with default parameters\") as run:\n",
    "    \n",
    "    # Log dataset information\n",
    "    log_dataset_info(train_df, name=\"train\", description=\"Training data for baseline model\")\n",
    "    log_dataset_info(val_df, name=\"validation\", description=\"Validation data for baseline model\")\n",
    "    \n",
    "    # Define model with default parameters\n",
    "    model = LogisticRegression(\n",
    "        random_state=42,\n",
    "        max_iter=1000  # Ensure convergence\n",
    "    )\n",
    "    \n",
    "    # Log model parameters\n",
    "    model_params = {\n",
    "        \"model_type\": \"LogisticRegression\",\n",
    "        \"penalty\": model.penalty,\n",
    "        \"C\": model.C,\n",
    "        \"solver\": model.solver,\n",
    "        \"max_iter\": model.max_iter,\n",
    "        \"random_state\": model.random_state\n",
    "    }\n",
    "    log_params(model_params)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training Logistic Regression model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Training complete.\")\n",
    "    \n",
    "    # Make predictions on training set\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_train_prob = model.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    # Make predictions on validation set\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_prob = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Log training metrics\n",
    "    print(\"\\n=== Training Set Performance ===\")\n",
    "    train_metrics = log_classification_metrics(y_train, y_train_pred, y_train_prob, prefix=\"train_\")\n",
    "    for metric, value in train_metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Log validation metrics\n",
    "    print(\"\\n=== Validation Set Performance ===\")\n",
    "    val_metrics = log_classification_metrics(y_val, y_val_pred, y_val_prob, prefix=\"val_\")\n",
    "    for metric, value in val_metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Store the run ID for reference\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"\\nMLflow Run ID: {run_id}\")\n",
    "    \n",
    "    # Log the model\n",
    "    log_model(model, artifact_path=\"model\", input_example=X_train[:5])\n",
    "    print(\"Model logged to MLflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "# Create confusion matrix visualization\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=['No Churn (0)', 'Churn (1)'],\n",
    "    yticklabels=['No Churn (0)', 'Churn (1)'],\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax.set_ylabel('True Label', fontsize=12)\n",
    "ax.set_title('Confusion Matrix - Baseline Logistic Regression (Validation Set)', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report (Validation Set):\")\n",
    "print(classification_report(y_val, y_val_pred, target_names=['No Churn', 'Churn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_val_prob)\n",
    "roc_auc = roc_auc_score(y_val, y_val_prob)\n",
    "\n",
    "# Create ROC curve visualization\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curve - Baseline Logistic Regression (Validation Set)', fontsize=14)\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "# Add threshold annotations\n",
    "ax.axhline(y=0.75, color='green', linestyle=':', alpha=0.5, label='Target TPR')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nROC-AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Baseline Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print baseline summary\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: Logistic Regression (default parameters)\")\n",
    "print(f\"Experiment: {experiment_name}\")\n",
    "print(f\"Run ID: {run_id}\")\n",
    "\n",
    "print(f\"\\n--- Validation Set Metrics ---\")\n",
    "print(f\"Accuracy:  {val_metrics['val_accuracy']:.4f}\")\n",
    "print(f\"Precision: {val_metrics['val_precision']:.4f}\")\n",
    "print(f\"Recall:    {val_metrics['val_recall']:.4f}\")\n",
    "print(f\"F1 Score:  {val_metrics['val_f1_score']:.4f}\")\n",
    "print(f\"ROC-AUC:   {val_metrics['val_roc_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\n--- Success Criteria Check ---\")\n",
    "baseline_threshold = 0.60\n",
    "target_threshold = 0.75\n",
    "\n",
    "if val_metrics['val_roc_auc'] >= baseline_threshold:\n",
    "    print(f\"[PASS] Baseline achieved AUC-ROC >= {baseline_threshold}\")\n",
    "else:\n",
    "    print(f\"[FAIL] Baseline did not achieve AUC-ROC >= {baseline_threshold}\")\n",
    "\n",
    "if val_metrics['val_roc_auc'] >= target_threshold:\n",
    "    print(f\"[PASS] Already meets final target AUC-ROC >= {target_threshold}!\")\n",
    "else:\n",
    "    improvement_needed = target_threshold - val_metrics['val_roc_auc']\n",
    "    print(f\"[INFO] Need {improvement_needed:.4f} improvement to reach target {target_threshold}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance (Coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get feature names from preprocessed data (excluding target and id columns)\nfeature_names = [col for col in train_preprocessed.columns \n                 if col not in ['churn', 'customer_id']]\n\n# Get feature coefficients\ncoefficients = pd.DataFrame({\n    'feature': feature_names,\n    'coefficient': model.coef_[0]\n}).sort_values('coefficient', key=abs, ascending=False)\n\nprint(\"Feature Coefficients (sorted by absolute value):\")\nprint(coefficients.to_string(index=False))\n\n# Visualize feature importance\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncolors = ['red' if c > 0 else 'blue' for c in coefficients['coefficient']]\nax.barh(coefficients['feature'], coefficients['coefficient'], color=colors, alpha=0.7)\n\nax.set_xlabel('Coefficient Value', fontsize=12)\nax.set_ylabel('Feature', fontsize=12)\nax.set_title('Logistic Regression Coefficients\\n(Red = increases churn probability, Blue = decreases)', fontsize=14)\nax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Misclassification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassifications\n",
    "val_results = val_df.copy()\n",
    "val_results['predicted'] = y_val_pred\n",
    "val_results['probability'] = y_val_prob\n",
    "val_results['correct'] = (val_results['churn'] == val_results['predicted'])\n",
    "\n",
    "# Categorize predictions\n",
    "val_results['prediction_type'] = 'TN'  # True Negative\n",
    "val_results.loc[(val_results['churn'] == 1) & (val_results['predicted'] == 1), 'prediction_type'] = 'TP'\n",
    "val_results.loc[(val_results['churn'] == 0) & (val_results['predicted'] == 1), 'prediction_type'] = 'FP'\n",
    "val_results.loc[(val_results['churn'] == 1) & (val_results['predicted'] == 0), 'prediction_type'] = 'FN'\n",
    "\n",
    "# Count each type\n",
    "prediction_counts = val_results['prediction_type'].value_counts()\n",
    "print(\"Prediction Type Counts:\")\n",
    "print(prediction_counts)\n",
    "\n",
    "# Analyze false negatives (churners we missed)\n",
    "false_negatives = val_results[val_results['prediction_type'] == 'FN']\n",
    "print(f\"\\n=== False Negatives ({len(false_negatives)} cases) ===\")\n",
    "print(\"Customers who churned but we predicted they wouldn't:\")\n",
    "if len(false_negatives) > 0:\n",
    "    print(f\"  Avg tenure: {false_negatives['tenure'].mean():.1f} months\")\n",
    "    print(f\"  Avg monthly charges: ${false_negatives['monthly_charges'].mean():.2f}\")\n",
    "    print(f\"  Contract types: {false_negatives['contract'].value_counts().to_dict()}\")\n",
    "\n",
    "# Analyze false positives (non-churners we flagged)\n",
    "false_positives = val_results[val_results['prediction_type'] == 'FP']\n",
    "print(f\"\\n=== False Positives ({len(false_positives)} cases) ===\")\n",
    "print(\"Customers who didn't churn but we predicted they would:\")\n",
    "if len(false_positives) > 0:\n",
    "    print(f\"  Avg tenure: {false_positives['tenure'].mean():.1f} months\")\n",
    "    print(f\"  Avg monthly charges: ${false_positives['monthly_charges'].mean():.2f}\")\n",
    "    print(f\"  Contract types: {false_positives['contract'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction probability distributions by actual class\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot distributions\n",
    "ax.hist(val_results[val_results['churn'] == 0]['probability'], \n",
    "        bins=20, alpha=0.6, label='Actual: No Churn', color='blue')\n",
    "ax.hist(val_results[val_results['churn'] == 1]['probability'], \n",
    "        bins=20, alpha=0.6, label='Actual: Churn', color='red')\n",
    "\n",
    "ax.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision threshold (0.5)')\n",
    "ax.set_xlabel('Predicted Churn Probability', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title('Distribution of Predicted Probabilities by Actual Class', fontsize=14)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate overlap metrics\n",
    "print(\"\\nProbability Distribution Statistics:\")\n",
    "print(f\"Non-churners - Mean prob: {val_results[val_results['churn']==0]['probability'].mean():.3f}, \"\n",
    "      f\"Std: {val_results[val_results['churn']==0]['probability'].std():.3f}\")\n",
    "print(f\"Churners     - Mean prob: {val_results[val_results['churn']==1]['probability'].mean():.3f}, \"\n",
    "      f\"Std: {val_results[val_results['churn']==1]['probability'].std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Potential Improvements\n",
    "\n",
    "Based on the baseline analysis, here are strategies to improve model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document improvement recommendations\n",
    "improvements = \"\"\"\n",
    "POTENTIAL IMPROVEMENTS FOR PHASE 3\n",
    "===================================\n",
    "\n",
    "1. MODEL COMPLEXITY\n",
    "   - Try tree-based models (Random Forest, Gradient Boosting) that can capture \n",
    "     non-linear relationships between features\n",
    "   - These models may better capture interaction effects between tenure, \n",
    "     contract type, and payment method\n",
    "\n",
    "2. FEATURE ENGINEERING\n",
    "   - Create interaction features (e.g., tenure * contract_type)\n",
    "   - Add polynomial features for numerical variables\n",
    "   - Create tenure buckets (new, medium, long-term customers)\n",
    "\n",
    "3. HYPERPARAMETER TUNING\n",
    "   - Tune regularization (C parameter) for Logistic Regression\n",
    "   - Use GridSearchCV or RandomizedSearchCV for systematic tuning\n",
    "   - Consider class weighting to handle imbalanced classes\n",
    "\n",
    "4. THRESHOLD OPTIMIZATION\n",
    "   - Current threshold is 0.5 (default)\n",
    "   - Optimize threshold based on business costs of FN vs FP\n",
    "   - If missing a churner is more costly, lower the threshold\n",
    "\n",
    "5. ENSEMBLE METHODS\n",
    "   - Combine predictions from multiple models\n",
    "   - Stack models with different strengths\n",
    "\n",
    "PRIORITY FOR PHASE 3:\n",
    "- [HIGH] Train Random Forest with hyperparameter tuning\n",
    "- [HIGH] Train Gradient Boosting with hyperparameter tuning  \n",
    "- [MEDIUM] Compare all models on validation set\n",
    "- [MEDIUM] Select best model and evaluate on test set\n",
    "\"\"\"\n",
    "\n",
    "print(improvements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Experiment Record\n",
    "\n",
    "This section documents the experiment for reproducibility and tracking:"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Create experiment record for documentation\nfrom datetime import datetime\n\nexperiment_record = f\"\"\"\n================================================================================\n                         EXPERIMENT RECORD\n================================================================================\n\nDate: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\nEXPERIMENT DETAILS\n------------------\nExperiment Name: {experiment_name}\nExperiment ID:   {experiment_id}\nRun ID:          {run_id}\n\nMODEL CONFIGURATION\n-------------------\nModel:           Logistic Regression\nPenalty:         L2 (default)\nRegularization:  C=1.0 (default)\nSolver:          lbfgs (default)\nRandom State:    42\n\nDATASET\n-------\nTotal Samples:   1000\nTraining:        700 (70%)\nValidation:      150 (15%)\nTest (held out): 150 (15%)\nChurn Rate:      20%\n\nVALIDATION RESULTS\n------------------\nAccuracy:        {val_metrics['val_accuracy']:.4f}\nPrecision:       {val_metrics['val_precision']:.4f}\nRecall:          {val_metrics['val_recall']:.4f}\nF1 Score:        {val_metrics['val_f1_score']:.4f}\nROC-AUC:         {val_metrics['val_roc_auc']:.4f}\n\nSUCCESS CRITERIA\n----------------\nBaseline Target (AUC > 0.60): {'PASS' if val_metrics['val_roc_auc'] >= 0.60 else 'FAIL'}\nFinal Target (AUC > 0.75):    {'PASS' if val_metrics['val_roc_auc'] >= 0.75 else 'PENDING'}\n\nKEY FINDINGS\n------------\n1. Logistic Regression provides a solid baseline for churn prediction\n2. Feature coefficients align with domain knowledge (tenure, contract type matter)\n3. Model shows reasonable class separation in probability distributions\n4. Room for improvement with more complex models in Phase 3\n\n================================================================================\n\"\"\"\n\nprint(experiment_record)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Next Steps\n\nThis baseline establishes the performance floor for our churn prediction model:\n\n1. **Phase 3 - Model Improvement**:\n   - Train Random Forest classifier with hyperparameter tuning\n   - Train Gradient Boosting classifier (XGBoost or sklearn)\n   - Compare all models on validation set\n   - Select best model based on ROC-AUC\n\n2. **Final Evaluation**:\n   - Evaluate best model on held-out test set (ONE TIME ONLY)\n   - Generate final metrics and feature importance\n   - Verify AUC-ROC > 0.75 target is met\n\n3. **Documentation**:\n   - Create model card with performance details\n   - Update project README\n\nThe baseline Logistic Regression model provides interpretable coefficients and a benchmark to beat with more complex models.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}